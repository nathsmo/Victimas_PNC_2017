{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Portafolio de algoritmos.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "W9WvayM3XPsK",
        "Wn0_2GxyC462",
        "9Neo-s4gHN-2",
        "931TPH3_HRIk",
        "QuTlKHmBXTK6",
        "PEv-GfynZchi",
        "TOugoMuZZc1K",
        "GzQaakhxZdG0",
        "6ky6r8pGZdY0",
        "soZj_sJFZdrG",
        "f9J_TMYdZd9l",
        "4F34C1OtF3rQ",
        "DgcVepWlNeCp",
        "F-CnMJiqNo97"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nathsmo/Victimas_PNC_2017/blob/master/Portafolio_de_algoritmos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Int7McLwXIEL",
        "colab_type": "text"
      },
      "source": [
        "# <center> Portafolio de Algoritmos </center>\n",
        "## <center> Nathalia Morales </center>\n",
        "### <center> Proyecto final Elements of Machine Learning - UFM - 2019</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9WvayM3XPsK",
        "colab_type": "text"
      },
      "source": [
        "# Librerías para los modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W9PayaFWnWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from IPython.display import Image\n",
        "from sklearn.metrics import mean_squared_error, confusion_matrix, classification_report\n",
        "from sklearn.externals.six import StringIO  \n",
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "import numpy as np  \n",
        "import pydotplus\n",
        "import matplotlib.pyplot as plt  \n",
        "from sklearn.model_selection import train_test_split  \n",
        "from sklearn.model_selection import cross_val_score\n",
        "from pandas import DataFrame\n",
        "import statsmodels.api as sm\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "import sklearn\n",
        "import matplotlib\n",
        "matplotlib.rcParams.update({'font.size': 12})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn0_2GxyC462",
        "colab_type": "text"
      },
      "source": [
        "# Función de tabla con los resultados de todos los modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAGOEol0C5CF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fct_names = ['Regresion Linear', 'Regresion Ridge', 'Analisis de Componentes Principales', 'Regresion Logistica',\n",
        "             'Analisis Discriminatorio Linear', 'Analisis Discriminatorio Cuadratico','K vecinos Cercanos', 'Random Forest Classifier']\n",
        "col_names = ['Modelo', 'Score/Resultado']\n",
        "classRegre = ['Regresion', 'Regresion', 'Regresion', 'Clasificacion', 'Clasificacion', 'Clasificacion', 'Clasificacion', 'Clasificacion']\n",
        "\n",
        "# X = todas las variables a evaluar\n",
        "# Y = la variable independiente a evaluar\n",
        "# i = componentes para el PCA\n",
        "# El máximo de los componentes evaluados es la cantidad de features (x) evaluadas dentro del modelo\n",
        "# n = numero de vecinos cercanos a evaluar\n",
        "# a = alpha a evaluar en la regresión Ridge\n",
        "# mf = max features en el modelo de random forest\n",
        "\n",
        "def t_funciones(x, y, i, n, a, mf):\n",
        "  resultados = []\n",
        "  X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.45, random_state = 42)\n",
        "  \n",
        "  # Regresion Linear\n",
        "  lm = LinearRegression()\n",
        "  lm = lm.fit(X_train, Y_train)\n",
        "  resultados.append(lm.score(X_test, Y_test))\n",
        "  \n",
        "  #Regresion Ridge\n",
        "  rr = Ridge(alpha = a)\n",
        "  rr.fit(X_train, Y_train)\n",
        "  resultados.append(rr.score(X_test, Y_test))\n",
        "  \n",
        "  #Analisis de Componentes Principales PCA\n",
        "  pca = PCA(n_components=i)\n",
        "  sc = StandardScaler()  \n",
        "  X_train = sc.fit_transform(X_train)  \n",
        "  X_test = sc.transform(X_test)\n",
        "  pca.fit(X_train, Y_train)\n",
        "  resultados.append(pca.score(X_test, Y_test))\n",
        "  \n",
        "  # Regresion Logistica\n",
        "  logisticRegr = LogisticRegression()\n",
        "  logisticRegr.fit(X_train, Y_train)\n",
        "  resultados.append(logisticRegr.score(X_test, Y_test))\n",
        "\n",
        "  #Analisis Discriminatorio Linear\n",
        "  LDA = LinearDiscriminantAnalysis(n_components=i)\n",
        "  sc = StandardScaler()  \n",
        "  X_train = sc.fit_transform(X_train)  \n",
        "  X_test = sc.transform(X_test)\n",
        "  LDA.fit(X_train, Y_train)\n",
        "  resultados.append(LDA.score(X_test, Y_test))\n",
        "  \n",
        "  #Analisis Discriminatorio Cuadratico\n",
        "  qda = QuadraticDiscriminantAnalysis()\n",
        "  qda.fit(X_train, Y_train)\n",
        "  resultados.append(qda.score(X_test, Y_test))\n",
        "\n",
        "\n",
        "  # K Vecinos cercanos\n",
        "  knn = KNeighborsClassifier(n_neighbors=n)  \n",
        "  knn.fit(X_train, Y_train)  \n",
        "  resultados.append(knn.score(X_test, Y_test))\n",
        "  \n",
        "  # Random Forest Classifier\n",
        "  rfr = RandomForestClassifier(max_features=mf, random_state=42)\n",
        "  rfr.fit(X_train, Y_train)\n",
        "  resultados.append(rfr.score(X_test, Y_test))\n",
        "\n",
        "\n",
        "    \n",
        "  resultados_finales = {'Modelo' : fct_names, 'Score/Resultado' : resultados, 'Mejor para:' : classRegre}\n",
        "  \n",
        "  tabla_final = pd.DataFrame(resultados_finales)\n",
        "  print(\"\")\n",
        "  return(tabla_final)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Neo-s4gHN-2",
        "colab_type": "text"
      },
      "source": [
        "# Función para Modelos de Regresión"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLh9SRucHYEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fct_names = ['Regresion Linear', 'Regresion Ridge', 'Analisis de Componentes Principales']\n",
        "col_names = ['Modelo', 'Score/Resultado']\n",
        "\n",
        "# X = todas las variables a evaluar\n",
        "# Y = la variable independiente a evaluar\n",
        "# i = componentes para el PCA\n",
        "# El máximo de los componentes evaluados es la cantidad de features (x) evaluadas dentro del modelo\n",
        "# a = alpha a evaluar en la regresión Ridge\n",
        "\n",
        "def fct_reg(x, y, i, a):\n",
        "  resultados = []\n",
        "  X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.45, random_state = 42)\n",
        "  \n",
        "  # Regresion Linear\n",
        "  lm = LinearRegression()\n",
        "  lm = lm.fit(X_train, Y_train)\n",
        "  resultados.append(lm.score(X_test, Y_test))\n",
        "  \n",
        "  #Regresion Ridge\n",
        "  rr = Ridge(alpha = a)\n",
        "  rr.fit(X_train, Y_train)\n",
        "  resultados.append(rr.score(X_test, Y_test))\n",
        "  \n",
        "  #Analisis de Componentes Principales PCA\n",
        "  pca = PCA(n_components=i)\n",
        "  sc = StandardScaler()  \n",
        "  X_train = sc.fit_transform(X_train)  \n",
        "  X_test = sc.transform(X_test)\n",
        "  pca.fit(X_train, Y_train)\n",
        "  resultados.append(pca.score(X_test, Y_test))\n",
        "  \n",
        "  resultados_finales = {'Score/Resultado' : resultados, 'Modelo:' : fct_names}\n",
        "  \n",
        "  tabla_final = pd.DataFrame(resultados_finales)\n",
        "  print(\"\")\n",
        "  return(tabla_final)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "931TPH3_HRIk",
        "colab_type": "text"
      },
      "source": [
        "# Función para Modelos de Clasificación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2bstBQ1H9Ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fct_names = ['Regresion Logistica', 'Analisis Discriminatorio Linear', 'Analisis Discriminatorio Cuadratico',\n",
        "             'K vecinos Cercanos', 'Random Forest Classifier']\n",
        "\n",
        "# X = todas las variables a evaluar\n",
        "# Y = la variable independiente a evaluar\n",
        "# n = numero de vecinos cercanos a evaluar\n",
        "# mf = max features en el modelo de random forest\n",
        "# El máximo de los componentes evaluados es la cantidad de features (x) evaluadas dentro del modelo\n",
        "\n",
        "def fct_class(x, y, n, mf):\n",
        "  resultados = []\n",
        "  X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.45, random_state = 42)\n",
        "  \n",
        "  \n",
        "  # Regresion Logistica\n",
        "  logisticRegr = LogisticRegression()\n",
        "  logisticRegr.fit(X_train, Y_train)\n",
        "  resultados.append(logisticRegr.score(X_test, Y_test))\n",
        "\n",
        "  #Analisis Discriminatorio Linear\n",
        "  LDA = LinearDiscriminantAnalysis(n_components=i)\n",
        "  sc = StandardScaler()  \n",
        "  X_train = sc.fit_transform(X_train)  \n",
        "  X_test = sc.transform(X_test)\n",
        "  LDA.fit(X_train, Y_train)\n",
        "  resultados.append(LDA.score(X_test, Y_test))\n",
        "  \n",
        "  #Analisis Discriminatorio Cuadratico\n",
        "  qda = QuadraticDiscriminantAnalysis()\n",
        "  qda.fit(X_train, Y_train)\n",
        "  resultados.append(qda.score(X_test, Y_test))\n",
        "\n",
        "\n",
        "  # K Vecinos cercanos\n",
        "  knn = KNeighborsClassifier(n_neighbors=n)  \n",
        "  knn.fit(X_train, Y_train)  \n",
        "  resultados.append(knn.score(X_test, Y_test))\n",
        "  \n",
        "  # Random Forest Classifier\n",
        "  rfr = RandomForestClassifier(max_features=mf, random_state=42)\n",
        "  rfr.fit(X_train, Y_train)\n",
        "  resultados.append(rfr.score(X_test, Y_test))\n",
        "\n",
        "\n",
        "  resultados_finales = {'Modelo' : fct_names, 'Score/Resultado' : resultados}\n",
        "  \n",
        "  tabla_final = pd.DataFrame(resultados_finales)\n",
        "  print(\"\")\n",
        "  return(tabla_final)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuTlKHmBXTK6",
        "colab_type": "text"
      },
      "source": [
        "# Función de Regresión Lineal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHSHtbMiXSf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def regresion_lineal(x, y):\n",
        "  X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.3, random_state = 42)\n",
        "  lm = LinearRegression()\n",
        "  lm = lm.fit(X_train, Y_train)\n",
        "  pred_train = lm.predict(X_train)\n",
        "  pred_test = lm.predict(X_test)\n",
        "  print(\"Model fitted with X_train, and calculated MSE with Y_train: %0.5f\" % (np.mean((Y_train - lm.predict(X_train)) ** 2)))\n",
        "  print(\"Model fitted with X_test, and calculated MSE with Y_test:   %0.5f\" % (np.mean((Y_test - lm.predict(X_test)) ** 2)))\n",
        "  scores = cross_val_score(lm, x, y, cv=5)\n",
        "  print(\"Calculated Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
        "  print(\"Model score: %0.2f\" % (lm.score(X_test, Y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEv-GfynZchi",
        "colab_type": "text"
      },
      "source": [
        "# Función de Regresión Logística"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyXEX0-yZcsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def regresion_logistica(x,y):\n",
        "  logisticRegr = LogisticRegression()\n",
        "  X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.3, random_state = 42)\n",
        "  logisticRegr.fit(X_train, Y_train)\n",
        "  predictions = logisticRegr.predict(X_test)\n",
        "  score = logisticRegr.score(X_test, Y_test)\n",
        "  print(\"\")\n",
        "  print(\"El score del modelo es: %0.5f\" % score)\n",
        "  cm = metrics.confusion_matrix(Y_test, predictions)\n",
        "  plt.figure(figsize=(9,9))\n",
        "  sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
        "  plt.ylabel('Actual label');\n",
        "  plt.xlabel('Predicted label');\n",
        "  all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
        "  plt.title(all_sample_title, size = 15);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOugoMuZZc1K",
        "colab_type": "text"
      },
      "source": [
        "# Función del Análisis Discriminatorio Linear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVcCzzc4Zc95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lda_analysis(x, y,i):\n",
        "  LDA = LinearDiscriminantAnalysis(n_components=i)\n",
        "  X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.3, random_state = 42)\n",
        "  sc = StandardScaler()  \n",
        "  X_train = sc.fit_transform(X_train)  \n",
        "  X_test = sc.transform(X_test)\n",
        "  \n",
        "  LDA.fit(X_train, Y_train)\n",
        "  predictions = LDA.predict(X_test)\n",
        "  score = LDA.score(X_test, Y_test)\n",
        "  print(\"\")\n",
        "  print(\"El score del modelo es: %0.2f\" % score)\n",
        "  cm = metrics.confusion_matrix(Y_test, predictions)\n",
        "  plt.figure(figsize=(9,9))\n",
        "  sns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
        "  plt.ylabel('Actual label');\n",
        "  plt.xlabel('Predicted label');\n",
        "  all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
        "  plt.title(all_sample_title, size = 15);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzQaakhxZdG0",
        "colab_type": "text"
      },
      "source": [
        "# Función del Análisis del componente principal (PCA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF25htXOZdP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PC_list = ['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10','PC11','PC12','PC13','PC14','PC15','PC16','PC17','PC18','PC19','PC20']\n",
        "def pca_analysis(x, y, i):\n",
        "  pca = PCA(n_components=i)\n",
        "\n",
        "  X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.3, random_state = 42)\n",
        "  sc = StandardScaler()  \n",
        "  X_train = sc.fit_transform(X_train)  \n",
        "  X_test = sc.transform(X_test)\n",
        "  \n",
        "  pca.fit(X_train, Y_train)\n",
        "  score = pca.score(X_test, Y_test)\n",
        "  print(\"\")\n",
        "  print(\"El score del modelo es: %0.3f\" % score)\n",
        "  print(\"\")\n",
        "  \n",
        "  pca_df = pd.DataFrame({'var':pca.explained_variance_ratio_,\n",
        "             'PC':PC_list[0:i]})\n",
        "  \n",
        "  sns.barplot(x='PC',y=\"var\", \n",
        "           data=pca_df, color=\"c\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ky6r8pGZdY0",
        "colab_type": "text"
      },
      "source": [
        "# Función de K-Vecinos Cercanos y gráfica de evaluación de rango"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-8XEDPNZdhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def knearest(x, y, i):\n",
        "  X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.3, random_state = 42)\n",
        "  knn = KNeighborsClassifier(n_neighbors=i)  \n",
        "  knn.fit(X_train, Y_train)  \n",
        "  print(\"Model score: %0.2f\" % (knn.score(X_test, Y_test)))\n",
        "  return knn.score(X_test, Y_test)\n",
        "\n",
        "def knearestgraph(x, y, i):\n",
        "  X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.3, random_state = 42)\n",
        "  knn = KNeighborsClassifier(n_neighbors=i)  \n",
        "  knn.fit(X_train, Y_train)  \n",
        "  return knn.score(X_test, Y_test)\n",
        "\n",
        "def function1(x, y, i):\n",
        "  return knearestgraph(x,y,i)\n",
        "\n",
        "def graphknn(x, y, t, l):\n",
        "  for i in range(len(t)):\n",
        "    l[i] = function1(x, y, t[i])\n",
        "\n",
        "  plot(t, l)\n",
        "  show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soZj_sJFZdrG",
        "colab_type": "text"
      },
      "source": [
        "# Función de Regresión Ridge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWdbsvD_ZdzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ridge_regr_w_lr(x, y):\n",
        "  X_train,X_test,y_train,y_test=train_test_split(x, y, test_size=0.3,random_state=42)\n",
        "\n",
        "  # Linear Regression\n",
        "  lr = LinearRegression()\n",
        "  lr.fit(X_train, y_train)\n",
        "\n",
        "  train_score=lr.score(X_train, y_train)\n",
        "  test_score=lr.score(X_test, y_test)\n",
        "\n",
        "  print \"Linear Regression train score:\", train_score\n",
        "  print \"Linear Regression test score: \", test_score\n",
        "\n",
        "\n",
        "  #Ridge Regression de 0.01\n",
        "  rr = Ridge(alpha=0.01) # higher the alpha value, more restriction on the coefficients; low alpha > more generalization, coefficients are barely restricted and in this case linear and ridge regression resembles\n",
        "  rr.fit(X_train, y_train)\n",
        "\n",
        "  Ridge_train_score = rr.score(X_train,y_train)\n",
        "  Ridge_test_score = rr.score(X_test, y_test)\n",
        "\n",
        "  print \"\"\n",
        "  print \"Ridge Regression train score low alpha:\", Ridge_train_score\n",
        "  print \"Ridge Regression test score low alpha: \", Ridge_test_score\n",
        "\n",
        "\n",
        "  #Ridge Regression de 100\n",
        "  rr100 = Ridge(alpha=100) # comparison with alpha value\n",
        "  rr100.fit(X_train, y_train)\n",
        "\n",
        "  Ridge_train_score100 = rr100.score(X_train,y_train)\n",
        "  Ridge_test_score100 = rr100.score(X_test, y_test)\n",
        "\n",
        "  print \"\"\n",
        "  print \"Ridge Regression train score high alpha:\", Ridge_train_score100\n",
        "  print \"Ridge Regression test score high alpha: \", Ridge_test_score100\n",
        "  print \"\"\n",
        "\n",
        "  #Plotting the graph\n",
        "  plt.plot(rr.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Ridge; $\\alpha = 0.01$',zorder=7) # zorder for ordering the markers\n",
        "  plt.plot(rr100.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Ridge; $\\alpha = 100$') # alpha here is for transparency\n",
        "  plt.plot(lr.coef_,alpha=0.4,linestyle='none',marker='o',markersize=7,color='green',label='Linear Regression')\n",
        "  plt.xlabel('Coefficient Index',fontsize=16)\n",
        "  plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
        "  plt.legend(fontsize=13,loc=4)\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9J_TMYdZd9l",
        "colab_type": "text"
      },
      "source": [
        "# Función del Análisis Discriminatorio cuadrático"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSONyycwZeGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def qda_analysis(x, y):\n",
        "  X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.3, random_state = 42)\n",
        "  qda = QuadraticDiscriminantAnalysis()\n",
        "  score = qda.score(X_test, Y_test)\n",
        "  print(\"El score del modelo es: %0.2f\" % score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F34C1OtF3rQ",
        "colab_type": "text"
      },
      "source": [
        "# Función de Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFEYidpSF3yT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_tree_class(x, y, mf): #max features\n",
        "  X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.3, random_state = 42)\n",
        "  rfc = RandomForestClassifier(max_features=mf, random_state=42)\n",
        "  rfc.fit(X_train, Y_train)\n",
        "  score = rfc.score(X_test, Y_test)\n",
        "  print(\"El score del modelo es: %0.2f\" % score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgcVepWlNeCp",
        "colab_type": "text"
      },
      "source": [
        "# Funcion de Random Forest Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL2Yl1ve_Cku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_forest_graph(x, y, mf):\n",
        "  X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.3, random_state = 42)\n",
        "  rfc = RandomForestClassifier(max_features=mf, random_state=42)\n",
        "  rfc.fit(X_train, Y_train)\n",
        "  Importance = pd.DataFrame({'Importance':rfc.feature_importances_*100/rfc.feature_importances_.max()}, index=x)\n",
        "  Importance.sort_values('Importance', axis=0, ascending=True).plot(kind='barh', color='r', )\n",
        "  plt.xlabel('Variable Importance')\n",
        "  plt.gca().legend_ = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-CnMJiqNo97",
        "colab_type": "text"
      },
      "source": [
        "# Funcion de Gradient Boosting Graph de Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gG5-DcqAQed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_boosting(x, y, ne, lr):\n",
        "  X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.3, random_state = 42)\n",
        "  regr = GradientBoostingClassifier(n_estimators=ne, learning_rate=lr, random_state=42)\n",
        "  regr.fit(X_train, Y_train)\n",
        "  feature_importance = regr.feature_importances_*100\n",
        "  rel_imp = pd.Series(feature_importance/regr.feature_importances_.max(), index=x).sort_values(inplace=False)\n",
        "  rel_imp.T.plot(kind='barh', color='r')\n",
        "  plt.xlabel('Variable Importance')\n",
        "  plt.gca().legend_ = None"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}